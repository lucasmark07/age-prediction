{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NHANES Age Prediction: Senior vs. Adult\n",
    "This notebook details the process of building a machine learning model to predict whether an individual from the National Health and Nutrition Examination Survey (NHANES) dataset is a \'Senior\' (65 years or older) or an \'Adult\' (under 65 years old)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration\n",
    "First, we load the training data and perform an initial exploration to understand its structure, identify missing values, and get a statistical summary.\n",
    "**Note:** Please ensure `Train_Data.csv` and `Test_Data.csv` are in the same directory as this notebook, or update the file paths accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "import joblib\n",
    "\n",
    "# Load the training data\n",
    "try:\n",
    "    train_df = pd.read_csv("Train_Data.csv")\n",
    "except FileNotFoundError:\n",
    "    print("Error: Train_Data.csv not found. Please ensure it's in the same directory as the notebook.")\n",
    "    exit()\n",
    "\n",
    "print(\'Shape of the training dataset:\', train_df.shape)\n",
    "print(\'\\nFirst 5 rows of the training dataset:\')\n",
    "print(train_df.head())\n",
    "print(\'\\nInformation about the training dataset:\')\n",
    "print(train_df.info())\n",
    "print(\'\\nDescriptive statistics of the training dataset:\')\n",
    "print(train_df.describe())\n",
    "print(\'\\nMissing values in each column of the training dataset:\')\n",
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Feature Engineering\n",
    "This section handles missing values through imputation and transforms categorical features using one-hot encoding. We also create the target variable `is_senior`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, is_train=True):\n",
    "    original_seq_numbers = None\n",
    "    if not is_train and \"SEQN\" in df.columns:\n",
    "        original_seq_numbers = df[\"SEQN\"]\n",
    "\n",
    "    categorical_features = [\"RIAGENDR\", \"PAQ605\", \"DIQ010\"]\n",
    "\n",
    "    numerical_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\n",
    "    if \"SEQN\" in numerical_cols: numerical_cols.remove(\"SEQN\")\n",
    "\n",
    "    imputer_numerical = SimpleImputer(strategy=\'mean\')\n",
    "    df[numerical_cols] = imputer_numerical.fit_transform(df[numerical_cols])\n",
    "\n",
    "    for col in categorical_features:\n",
    "        if col in df.columns:\n",
    "            imputer_categorical = SimpleImputer(strategy=\'most_frequent\')\n",
    "            df[col] = imputer_categorical.fit_transform(df[[col]])\n",
    "            df[col] = df[col].astype(int).astype(str)\n",
    "\n",
    "    if is_train:\n",
    "        df[\"is_senior\"] = (df[\"RIDAGEYR\"] >= 65).astype(int)\n",
    "        if \'age_group\' in df.columns:\n",
    "            df.drop(columns=[\'age_group\'], inplace=True)\n",
    "\n",
    "    df = pd.get_dummies(df, columns=categorical_features, drop_first=True)\n",
    "\n",
    "    if \"SEQN\" in df.columns:\n",
    "        df.drop(columns=[\"SEQN\"], inplace=True)\n",
    "\n",
    "    if not is_train:\n",
    "        return df, original_seq_numbers\n",
    "    return df\n",
    "\n",
    "processed_train_df = preprocess_data(train_df.copy(), is_train=True)\n",
    "processed_train_df.to_csv(\"processed_train_data.csv\", index=False)\n",
    "\n",
    "print(\"Shape of the processed training dataset:\", processed_train_df.shape)\n",
    "print(\"\\nFirst 5 rows of the processed training dataset:\")\n",
    "print(processed_train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training and Evaluation (XGBoost with Hyperparameter Tuning)\n",
    "We train an XGBoost Classifier, which is known for its strong performance, and use GridSearchCV to find the optimal hyperparameters. The model is then evaluated on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = processed_train_df.drop(columns=[\"is_senior\"])\n",
    "y = processed_train_df[\"is_senior\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Removed use_label_encoder as it's deprecated and not needed with one-hot encoding\n",
    "model = XGBClassifier(random_state=42, eval_metric=\'logloss\')\n",
    "\n",
    "param_grid = {\n",
    "    \'n_estimators\': [100, 200, 300],\n",
    "    \'max_depth\': [3, 5, 7],\n",
    "    \'learning_rate\': [0.01, 0.1, 0.2],\n",
    "    \'subsample\': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring=\'accuracy\', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "joblib.dump(best_model, \'xgboost_model.pkl\')\n",
    "print(\"Model saved as xgboost_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Analysis and Visualization\n",
    "We visualize the model\'s performance using a Confusion Matrix and an ROC Curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp_cm = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Not Senior\", \"Senior\"])\n",
    "disp_cm.plot()\n",
    "plt.title(\"Confusion Matrix - XGBoost\")\n",
    "plt.savefig(\"confusion_matrix_xgb.png\")\n",
    "plt.close()\n",
    "print(\"Confusion matrix saved as confusion_matrix_xgb.png\")\n",
    "\n",
    "disp_roc = RocCurveDisplay.from_estimator(best_model, X_test, y_test)\n",
    "plt.title(\"ROC Curve - XGBoost\")\n",
    "plt.savefig(\"roc_curve_xgb.png\")\n",
    "plt.close()\n",
    "print(\"ROC curve saved as roc_curve_xgb.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Submission File\n",
    "Finally, we load the test data, preprocess it using the same steps as the training data, and generate predictions to create the submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    test_df = pd.read_csv("Test_Data.csv")\n",
    "except FileNotFoundError:\n",
    "    print("Error: Test_Data.csv not found. Please ensure it's in the same directory as the notebook.")\n",
    "    exit()\n",
    "\n",
    "processed_test_df, original_test_seq_numbers = preprocess_data(test_df.copy(), is_train=False)\n",
    "\n",
    "# Ensure test set columns match training set columns after one-hot encoding\n",
    "# This is crucial if some categories are missing in the test set or vice-versa\n",
    "train_cols = processed_train_df.drop(columns=[\"is_senior\"]).columns\n",
    "processed_test_df = processed_test_df.reindex(columns=train_cols, fill_value=0)\n",
    "\n",
    "predictions = best_model.predict(processed_test_df)\n",
    "\n",
    "submission_df = pd.DataFrame({\"SEQN\": original_test_seq_numbers, \"age_group\": predictions})\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Submission file generated as submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Summary\n",
    "This submission presents a machine learning model to predict whether an individual is a \'Senior\' (65+) or \'Adult\' (under 65) based on the National Health and Nutrition Examination Survey (NHANES) dataset. The process involved robust data preprocessing, including imputation of missing values and one-hot encoding of categorical features. An XGBoost Classifier was trained with hyperparameter tuning, demonstrating strong internal validation metrics (Accuracy, Precision, Recall, F1-Score, and ROC AUC all at 1.0000). The `submission.csv` file contains the model\'s predictions for the test set, formatted as required for the hackathon."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

